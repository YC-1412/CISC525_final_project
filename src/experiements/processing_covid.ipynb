{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing COVID cases\n",
    "The input files are daily cumulative COVID Confirmed, Deaths, Recovered, and Active case count by country, province and administration area. This script load in the daily file, aggregate it to the country-month level, producing both cumulative and newly COVID Confirmed, Deaths, Recovered, and Active case count.\n",
    "- Data comes from [COVID-19 case data from Johns Hopkins CSSE](https://github.com/CSSEGISandData/COVID-19).\n",
    "- Data cleaning\n",
    "    - There are some daily files that does not come with the consistent schema, e.g. missing `Admin2` and `Province_State`, and have `Country/Region` instead of `Country_Region`. Those are a small portion of the data and hard to identify because all daily file has the same name. We just skip those files in this analysis.\n",
    "    - There are 487 (0.011%) records with abnormal dates, e.g. 2682-01-01, or `Country_Region` name as numbers. Those records are dropped.\n",
    "    - The daily file could have records not on that date. For example, a 01-01-2021 file can have records for 2020 or Apr 2021, maybe because it is the last updated date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a spark session\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"COVID-19 and Flight Volume Analysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+--------------+--------------+-------------------+--------+--------+---------+------+---------+------+------------+-------------+-------------------+\n",
      "|FIPS|Admin2|Province_State|Country_Region|        Last_Update|     Lat|   Long_|Confirmed|Deaths|Recovered|Active|Combined_Key|Incident_Rate|Case_Fatality_Ratio|\n",
      "+----+------+--------------+--------------+-------------------+--------+--------+---------+------+---------+------+------------+-------------+-------------------+\n",
      "|NULL|  NULL|          NULL|   Afghanistan|2021-02-01 05:22:53|33.93911|67.70995|    55023|  2400|    47679|  4944| Afghanistan|    141.34433|           4.361812|\n",
      "|NULL|  NULL|          NULL|       Albania|2021-02-01 05:22:53| 41.1533| 20.1683|    78127|  1380|    47424| 29323|     Albania|     2714.817|          1.7663548|\n",
      "|NULL|  NULL|          NULL|       Algeria|2021-02-01 05:22:53| 28.0339|  1.6596|   107339|  2891|    73344| 31104|     Algeria|    244.78094|           2.693336|\n",
      "|NULL|  NULL|          NULL|       Andorra|2021-02-01 05:22:53| 42.5063|  1.5218|     9937|   101|     9093|   743|     Andorra|    12860.934|          1.0164033|\n",
      "|NULL|  NULL|          NULL|        Angola|2021-02-01 05:22:53|-11.2027| 17.8739|    19796|   466|    18035|  1295|      Angola|    60.231968|          2.3540108|\n",
      "+----+------+--------------+--------------+-------------------+--------+--------+---------+------+---------+------+------------+-------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_covid_data(data_path: str, batch_date: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Load the covid data for a given batch date\n",
    "    \"\"\"\n",
    "    schema = StructType([\n",
    "        StructField(\"FIPS\", StringType(), True),\n",
    "        StructField(\"Admin2\", StringType(), True),\n",
    "        StructField(\"Province_State\", StringType(), True),\n",
    "        StructField(\"Country_Region\", StringType(), True),\n",
    "        StructField(\"Last_Update\", TimestampType(), True),\n",
    "        StructField(\"Lat\", FloatType(), True),\n",
    "        StructField(\"Long_\", FloatType(), True),\n",
    "        StructField(\"Confirmed\", IntegerType(), True),\n",
    "        StructField(\"Deaths\", IntegerType(), True),\n",
    "        StructField(\"Recovered\", IntegerType(), True),\n",
    "        StructField(\"Active\", IntegerType(), True),\n",
    "        StructField(\"Combined_Key\", StringType(), True),\n",
    "        StructField(\"Incident_Rate\", FloatType(), True),\n",
    "        StructField(\"Case_Fatality_Ratio\", FloatType(), True),\n",
    "    ]\n",
    "    )\n",
    "    \n",
    "    df_covid = spark.read.csv(f\"{data_path}/{batch_date}.csv\", header=True, schema=schema, mode='DROPMALFORMED')\n",
    "    return df_covid\n",
    "\n",
    "# load all csv files from data/csse_covid_19_daily_reports\n",
    "df_covid = load_covid_data('../data/csse_covid_19_daily_reports', '01-*-2021')\n",
    "df_covid.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning message like below is because some daily data has a different header. Those fail will fail to load\n",
    "\n",
    "```log\n",
    "25/04/05 16:45:55 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
    " Header: Country/Region, Last Update, Confirmed, Deaths, , , , \n",
    " Schema: Admin2, Province_State, Country_Region, Last_Update, Confirmed, Deaths, Recovered, Active\n",
    "Expected: Admin2 but found: Country/Region\n",
    "CSV file: file:///Users/yc/Documents/HU/CISC_525_Big_Data_Architecture/CISC525_final_project/data/csse_covid_19_daily_reports/02-26-2020.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:========== date is missing or abnormal: ==========\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----------------------+\n",
      "|date Missing Abnormal|date Missing Abnormal %|\n",
      "+---------------------+-----------------------+\n",
      "|                  487|   0.011442357304171221|\n",
      "+---------------------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Dropping rows with missing or abnormal date                       \n",
      "INFO:__main__:========== Aggregate by date: ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+-------------------+--------------+---------+------+---------+------+\n",
      "|      date|        Admin2|     Province_State|Country_Region|Confirmed|Deaths|Recovered|Active|\n",
      "+----------+--------------+-------------------+--------------+---------+------+---------+------+\n",
      "|2727-01-01|Mainland China|2020-02-29T12:13:10|         66337|     NULL|  NULL|     NULL|  NULL|\n",
      "|2682-01-01|Mainland China|2020-02-28T00:43:01|         65914|     NULL|  NULL|     NULL|  NULL|\n",
      "|2641-01-01|Mainland China|2020-02-27T12:13:14|         65596|     NULL|  NULL|     NULL|  NULL|\n",
      "|2615-01-01|Mainland China|2020-02-26T14:13:10|         65187|     NULL|  NULL|     NULL|  NULL|\n",
      "|2563-01-01|Mainland China|2020-02-25T15:23:04|         64786|     NULL|  NULL|     NULL|  NULL|\n",
      "|2495-01-01|Mainland China|2020-02-24T11:13:09|         64287|     NULL|  NULL|     NULL|  NULL|\n",
      "|2346-01-01|Mainland China|2020-02-23T11:33:03|         64084|     NULL|  NULL|     NULL|  NULL|\n",
      "|2346-01-01|Mainland China|2020-02-22T23:33:06|         64084|     NULL|  NULL|     NULL|  NULL|\n",
      "|2144-01-01|Mainland China|2020-02-21T13:03:09|         62662|     NULL|  NULL|     NULL|  NULL|\n",
      "|2144-01-01|Mainland China|2020-02-20T23:43:02|         62442|     NULL|  NULL|     NULL|  NULL|\n",
      "+----------+--------------+-------------------+--------------+---------+------+---------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:========== Count na values: ==========                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------------+-----------------+--------------------+-----------------+-------------------+----------------+-------------------+----------------+\n",
      "|      date|Country_Region|Confirmed_cumulative|Deaths_cumulative|Recovered_cumulative|Active_cumulative|Confirmed_daily_new|Deaths_daily_new|Recovered_daily_new|Active_daily_new|\n",
      "+----------+--------------+--------------------+-----------------+--------------------+-----------------+-------------------+----------------+-------------------+----------------+\n",
      "|2020-02-23|         China|                 570|                0|                 570|                0|               NULL|            NULL|               NULL|            NULL|\n",
      "|2020-03-08|         China|               13170|              156|               13014|                0|              12600|             156|              12444|               0|\n",
      "|2020-03-12|         China|                 186|                2|                 184|                0|             -12984|            -154|             -12830|               0|\n",
      "|2020-03-13|         China|                 133|                0|                 133|                0|                -53|              -2|                -51|               0|\n",
      "|2020-03-14|         China|                7126|               28|                7098|                0|               6993|              28|               6965|               0|\n",
      "+----------+--------------+--------------------+-----------------+--------------------+-----------------+-------------------+----------------+-------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+--------------------+-----------------+--------------------+-----------------+-------------------+----------------+-------------------+----------------+\n",
      "|date|Country_Region|Confirmed_cumulative|Deaths_cumulative|Recovered_cumulative|Active_cumulative|Confirmed_daily_new|Deaths_daily_new|Recovered_daily_new|Active_daily_new|\n",
      "+----+--------------+--------------------+-----------------+--------------------+-----------------+-------------------+----------------+-------------------+----------------+\n",
      "|   0|             0|                   0|                0|              113621|           113621|                  1|               1|             116520|          116520|\n",
      "+----+--------------+--------------------+-----------------+--------------------+-----------------+-------------------+----------------+-------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# hide the warning\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "def process_daily_data(df: DataFrame, country: list[str] = None) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Process the daily data for given country.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame, the daily data\n",
    "        country: list[str], the country to process\n",
    "\n",
    "    Returns:\n",
    "        DataFrame, the daily data for the given country\n",
    "    \"\"\"\n",
    "    locations = ['Admin2', 'Province_State', 'Country_Region']\n",
    "    stats = ['Confirmed', 'Deaths', 'Recovered', 'Active']\n",
    "\n",
    "    if country is None:\n",
    "        df_daily = df\n",
    "    else:\n",
    "        df_daily = df.filter(df['Country_Region'].isin(country))\n",
    "\n",
    "    df_daily = df_daily.withColumn('date', F.date_format(F.col('Last_Update'), 'yyyy-MM-dd'))\n",
    "    df_daily = df_daily[['date'] + locations + stats]\n",
    "\n",
    "    # drop rows with missing date\n",
    "    logger.info(f\"{'='*10} date is missing or abnormal: {'='*10}\")\n",
    "    df_daily.select(\n",
    "        F.count(F.when(F.isnull('date') | (F.year('date') >= 2024) | (F.year('date') < 2020), 1)).alias('date Missing Abnormal'),\n",
    "        (F.count(F.when(F.isnull('date') | (F.year('date') >= 2024) | (F.year('date') < 2020), 1)) / F.count('*') * 100).alias('date Missing Abnormal %')\n",
    "    ).show()\n",
    "    df_daily.filter(\n",
    "        F.isnull('date') | (F.year('date') >= 2024) | (F.year('date') < 2020)\n",
    "    ).show(10)\n",
    "    \n",
    "    logger.info(f\"Dropping rows with missing or abnormal date\")\n",
    "    df_daily = df_daily.na.drop(subset=['date']).filter((F.year('date') >= 2020) & (F.year('date') <= 2024))\n",
    "\n",
    "    # aggregate by date and country\n",
    "    df_daily = df_daily.groupBy('date', 'Country_Region').agg(\n",
    "        *[F.sum(c).alias(c) for c in stats]\n",
    "    ).sort(['Country_Region', 'date'])\n",
    "\n",
    "    # get daily change from the previous day for Confirmed, Deaths, Recovered, Active\n",
    "    for stat in stats:\n",
    "        df_daily = df_daily.withColumn(f'{stat}_daily_new', \n",
    "                            F.col(stat) - F.lag(stat).over(Window.partitionBy().orderBy('date'))) \\\n",
    "                 .withColumnRenamed(stat, f'{stat}_cumulative')\n",
    "        \n",
    "    logger.info(f\"{'='*10} Aggregate by date: {'='*10}\")\n",
    "    df_daily.show(5)\n",
    "\n",
    "    # check missing\n",
    "    logger.info(f\"{'='*10} Count na values: {'='*10}\")\n",
    "    df_daily.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df_daily.columns]).show()\n",
    "    return df_daily\n",
    "\n",
    "df_daily = process_daily_data(df_covid, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------------+-----------------+--------------------+-----------------+---------------------+------------------+---------------------+------------------+\n",
      "|year_month|Country_Region|Confirmed_cumulative|Deaths_cumulative|Recovered_cumulative|Active_cumulative|Confirmed_monthly_new|Deaths_monthly_new|Recovered_monthly_new|Active_monthly_new|\n",
      "+----------+--------------+--------------------+-----------------+--------------------+-----------------+---------------------+------------------+---------------------+------------------+\n",
      "|   2020-03|   Afghanistan|                 166|                4|                   5|              157|                -8312|               -21|                -8672|               381|\n",
      "|   2020-04|   Afghanistan|                1827|               60|                 252|             1515|              -139574|             -4997|               -24743|           -109834|\n",
      "|   2020-05|   Afghanistan|               14529|              252|                1303|            12974|               207776|              4606|                22639|            180531|\n",
      "|   2020-06|   Afghanistan|               31324|              737|               13934|            20294|               724137|             14361|               170108|            539668|\n",
      "|   2020-07|   Afghanistan|               36628|             1275|               25471|            16575|              1031212|             32288|               665352|            333572|\n",
      "+----------+--------------+--------------------+-----------------+--------------------+-----------------+---------------------+------------------+---------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# year month from Last_Update\n",
    "def process_monthly_data(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Process the monthly data.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame, the daily data\n",
    "\n",
    "    Returns:\n",
    "        DataFrame, the monthly data\n",
    "    \"\"\"\n",
    "    locations = ['Admin2', 'Province_State', 'Country_Region']\n",
    "    stats = ['Confirmed', 'Deaths', 'Recovered', 'Active']\n",
    "    df_monthly = df.withColumn('year_month', F.date_format(F.col('date'), 'yyyy-MM'))\n",
    "    df_monthly = df_monthly.groupBy(['year_month', 'Country_Region']).agg(\n",
    "        *[F.max(c + '_cumulative').alias(c + '_cumulative') for c in stats],\n",
    "        *[F.sum(c + '_daily_new').alias(c + '_monthly_new') for c in stats]\n",
    "    ).sort(['Country_Region', 'year_month'])\n",
    "    return df_monthly\n",
    "\n",
    "df_monthly = process_monthly_data(df_daily)\n",
    "df_monthly.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
