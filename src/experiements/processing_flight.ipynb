{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing flight volume data\n",
    "The input files are daily flight information about flight origin and destination by airport. This script load in the data, map airport to country, and aggregate flight volume to the country-month level. It can either aggregate by origin or desination country.\n",
    "- Data comes from https://zenodo.org/records/7923702\n",
    "    - The monthly file could have records not in that month. For example, a Jan 2021 file can have records for 2020, reason unknown.\n",
    "- `airports.csv` comes from [https://github.com/mborsetti/airportsdata](https://github.com/mborsetti/airportsdata/blob/main/airportsdata/airports.csv)\n",
    "- `countries.csv` comes from [radcliff/wikipedia-iso-country-codes.csv](https://gist.github.com/radcliff/f09c0f88344a7fcef373#file-wikipedia-iso-country-codes-csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"COVID-19 and Flight Volume Analysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 179:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+-----------+\n",
      "|                day|origin|destination|\n",
      "+-------------------+------+-----------+\n",
      "|2018-12-31 19:00:00|  YMML|       LFPG|\n",
      "|2018-12-31 19:00:00|  YMML|       LEBL|\n",
      "|2018-12-31 19:00:00|  YSSY|       EDDF|\n",
      "|2018-12-31 19:00:00|  LEMD|       LEMD|\n",
      "|2018-12-31 19:00:00|  YSSY|       LFPG|\n",
      "+-------------------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----+----+--------------------+------------+-------+-------+---------+--------+----------+-----------------+----+\n",
      "|icao|iata|                name|        city|   subd|country|elevation|     lat|       lon|               tz| lid|\n",
      "+----+----+--------------------+------------+-------+-------+---------+--------+----------+-----------------+----+\n",
      "|00AA|NULL|Aero B Ranch Airport|       Leoti| Kansas|     US|   3435.0|38.70402|-101.47391|  America/Chicago|00AA|\n",
      "|00AK|NULL|        Lowell Field|Anchor Point| Alaska|     US|    252.0|59.94889|-151.69222|America/Anchorage|00AK|\n",
      "|00AL|NULL|        Epps Airpark|     Harvest|Alabama|     US|    820.0|34.86481| -86.77028|  America/Chicago|00AL|\n",
      "|00AN|NULL|Katmai Lodge Airport| King Salmon| Alaska|     US|     80.0|59.09347|-156.45583|America/Anchorage|00AN|\n",
      "|00AR|NULL|      Arland Airport|  Bennington| Kansas|     US|   1352.0|38.96965| -97.60156|  America/Chicago|00AR|\n",
      "+----+----+--------------------+------------+-------+-------+---------+--------+----------+-----------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# load csv.gz file in ../data/flight_volume_raw\n",
    "airports = spark.read.csv('../data/airports.csv', header=True, inferSchema=True)\n",
    "countries = spark.read.csv('../data/countries.csv', header=True, inferSchema=True) \\\n",
    "    .withColumnRenamed('English short name lower case', 'country') \\\n",
    "    .withColumnRenamed('Alpha-2 code', 'country_code') \\\n",
    "    .select('country', 'country_code')\n",
    "def load_flight_data(data_path: str, batch_date: str):\n",
    "    batch_date = batch_date[:6]   # only use year and month\n",
    "    df_flight = spark.read.csv(f'../data/flight_volume_raw/flightlist_{batch_date}*_{batch_date}*.csv.gz', header=True, inferSchema=True).select('day', 'origin', 'destination')\n",
    "    return df_flight\n",
    "\n",
    "df_flight = load_flight_data('../data/flight_volume_raw/flightlist_20190101_20190131.csv.gz', '201901')\n",
    "\n",
    "df_flight.show(5)\n",
    "airports.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------------+------------------------+-----------+----------------+--------------+-------------------+\n",
      "|       day|year_month|origin_country_code|destination_country_code|origin_icao|destination_icao|origin_country|destination_country|\n",
      "+----------+----------+-------------------+------------------------+-----------+----------------+--------------+-------------------+\n",
      "|2018-12-31|   2018-12|                 AU|                      FR|       YMML|            LFPG|     Australia|             France|\n",
      "|2018-12-31|   2018-12|                 AU|                      ES|       YMML|            LEBL|     Australia|              Spain|\n",
      "|2018-12-31|   2018-12|                 AU|                      DE|       YSSY|            EDDF|     Australia|            Germany|\n",
      "|2018-12-31|   2018-12|                 ES|                      ES|       LEMD|            LEMD|         Spain|              Spain|\n",
      "|2018-12-31|   2018-12|                 AU|                      FR|       YSSY|            LFPG|     Australia|             France|\n",
      "+----------+----------+-------------------+------------------------+-----------+----------------+--------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# map origin and destination to country\n",
    "def map_flight_data(df_flight: DataFrame, airports: DataFrame, countries: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Map the flight data to country and icao\n",
    "\n",
    "    Args:\n",
    "        df_flight: DataFrame, the flight data\n",
    "        airports: DataFrame, the airports data\n",
    "\n",
    "    Returns:\n",
    "        df_flight_mapped: DataFrame, the flight data with country and icao\n",
    "    \"\"\"\n",
    "    airports = airports.select('icao', 'country')\n",
    "    df_flight_mapped = df_flight.join(airports, df_flight['origin'] == airports['icao'], 'left')\\\n",
    "        .withColumnRenamed('country', 'origin_country').withColumnRenamed('icao', 'origin_icao') \\\n",
    "    .join(airports, df_flight['destination'] == airports['icao'], 'left') \\\n",
    "    .withColumnRenamed('country', 'destination_country').withColumnRenamed('icao', 'destination_icao') \\\n",
    "    .select('day', 'origin_country', 'destination_country', 'origin_icao', 'destination_icao')\n",
    "    df_flight_mapped = df_flight_mapped.withColumn('day', F.to_date('day')) \\\n",
    "        .withColumn('year_month', F.date_format('day', 'yyyy-MM')) \\\n",
    "        .select('day', 'year_month', 'origin_country', 'destination_country', 'origin_icao', 'destination_icao')\n",
    "    # map country abbreviation to country name\n",
    "    df_flight_mapped = df_flight_mapped.join(countries, df_flight_mapped['origin_country'] == countries['country_code'], 'left') \\\n",
    "        .drop('country_code') \\\n",
    "        .withColumnRenamed('origin_country', 'origin_country_code').withColumnRenamed('country', 'origin_country')\n",
    "    df_flight_mapped = df_flight_mapped.join(countries, df_flight_mapped['destination_country'] == countries['country_code'], 'left') \\\n",
    "        .drop('country_code') \\\n",
    "        .withColumnRenamed('destination_country', 'destination_country_code').withColumnRenamed('country', 'destination_country')\n",
    "    \n",
    "    # drop rows with missing country or date\n",
    "    df_flight_mapped = df_flight_mapped.filter(F.col('origin_country').isNotNull() & F.col('destination_country').isNotNull() & F.col('day').isNotNull())\n",
    "\n",
    "    return df_flight_mapped\n",
    "\n",
    "df_flight_mapped = map_flight_data(df_flight, airports, countries)\n",
    "\n",
    "df_flight_mapped.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------------+-------------------+------------------------+------------+\n",
      "|       day|      origin_country|origin_country_code|destination_country|destination_country_code|flight_count|\n",
      "+----------+--------------------+-------------------+-------------------+------------------------+------------+\n",
      "|2018-12-31|United Arab Emirates|                 AE|            Austria|                      AT|           2|\n",
      "|2019-01-01|United Arab Emirates|                 AE|            Austria|                      AT|           2|\n",
      "|2019-01-02|United Arab Emirates|                 AE|            Austria|                      AT|           3|\n",
      "|2019-01-03|United Arab Emirates|                 AE|            Austria|                      AT|           2|\n",
      "|2019-01-04|United Arab Emirates|                 AE|            Austria|                      AT|           2|\n",
      "+----------+--------------------+-------------------+-------------------+------------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 222:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------------+-------------------+------------------------+------------+\n",
      "|year_month|      origin_country|origin_country_code|destination_country|destination_country_code|flight_count|\n",
      "+----------+--------------------+-------------------+-------------------+------------------------+------------+\n",
      "|   2018-12|United Arab Emirates|                 AE|            Austria|                      AT|           2|\n",
      "|   2019-01|United Arab Emirates|                 AE|            Austria|                      AT|          70|\n",
      "|   2018-12|United Arab Emirates|                 AE|          Australia|                      AU|          13|\n",
      "|   2019-01|United Arab Emirates|                 AE|          Australia|                      AU|         474|\n",
      "|   2019-01|United Arab Emirates|                 AE|              Aruba|                      AW|           1|\n",
      "+----------+--------------------+-------------------+-------------------+------------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def process_flight_data(df: DataFrame, direction: str = 'origin', granular: str = 'day', country: list[str] = None) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Process the daily or monthly data\n",
    "    Args:\n",
    "        df: DataFrame, the flight data\n",
    "        direction: str, the direction of the flight\n",
    "        granular: str, the granularity of the data\n",
    "        country: list[str], the country to filter the data\n",
    "    Returns:\n",
    "        df_agg: DataFrame, the aggregated data\n",
    "    \"\"\"\n",
    "    assert direction in ['origin', 'destination']\n",
    "    assert granular in ['day', 'year_month']\n",
    "    opposite_direction = 'origin' if direction == 'destination' else 'destination'\n",
    "    if country:\n",
    "        df = df.filter(F.col(f'{direction}_country').isin(country))\n",
    "    # exclude domestic flight\n",
    "    df = df.filter(F.col('origin_country_code') != F.col('destination_country_code'))\n",
    "    # aggrgate by country and month or day\n",
    "    df_agg = df.groupBy(f'{granular}', 'origin_country', 'origin_country_code', 'destination_country', 'destination_country_code').agg(\n",
    "        F.count('*').alias(f'flight_count')\n",
    "    ).sort(f'{direction}_country_code', f'{opposite_direction}_country_code', f'{granular}')\n",
    "    return df_agg\n",
    "\n",
    "df_daily = process_flight_data(df_flight_mapped, 'origin', 'day')\n",
    "df_daily.show(5)\n",
    "\n",
    "df_monthly = process_flight_data(df_flight_mapped, 'origin', 'year_month')\n",
    "df_monthly.show(5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
